{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# requirements:\n",
    "numpy, pandas, nltk, scikit-learn, matplotlib, seaborn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import names\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: mcmullen gas for 11 / 99\n",
      "jackie ,\n",
      "since the inlet to 3 river plant is shut in on 10 / 19 / 99 ( the last day of\n",
      "flow ) :\n",
      "at what meter is the mcmullen gas being diverted to ?\n",
      "at what meter is hpl buying the residue gas ? ( this is the gas from teco ,\n",
      "vastar , vintage , tejones , and swift )\n",
      "i still see active deals at meter 3405 in path manager for teco , vastar ,\n",
      "vintage , tejones , and swift\n",
      "i also see gas scheduled in pops at meter 3404 and 3405 .\n",
      "please advice . we need to resolve this as soon as possible so settlement\n",
      "can send out payments .\n",
      "thanks\n",
      "Subject: stacey automated system generating 8 k per week parallelogram\n",
      "people are\n",
      "getting rich using this system ! now it ' s your\n",
      "turn !\n",
      "we ' ve\n",
      "cracked the code and will show you . . . .\n",
      "this is the\n",
      "only system that does everything for you , so you can make\n",
      "money\n",
      ". . . . . . . .\n",
      "because your\n",
      "success is . . . completely automated !\n",
      "let me show\n",
      "you how !\n",
      "click\n",
      "here\n",
      "to opt out click here % random _ text\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Test raw data\n",
    "file_path = 'enron1/ham/0007.1999-12-14.farmer.ham.txt'\n",
    "with open(file_path, 'r') as infile:\n",
    "    ham_sample = infile.read()\n",
    "print(ham_sample)\n",
    "\n",
    "file_path = 'enron1/spam/0058.2003-12-21.GP.spam.txt'\n",
    "with open(file_path, 'r') as infile:\n",
    "    spam_sample = infile.read()\n",
    "print(spam_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all data files\n",
    "emails, labels = [], []\n",
    "\n",
    "file_path = 'enron1/spam/'\n",
    "for filename in glob.glob(os.path.join(file_path, '*.txt')):\n",
    "    with open(filename, 'r', encoding = \"ISO-8859-1\") as infile:\n",
    "        emails.append(infile.read())\n",
    "        labels.append(1)\n",
    "\n",
    "file_path = 'enron1/ham/'\n",
    "for filename in glob.glob(os.path.join(file_path, '*.txt')):\n",
    "    with open(filename, 'r', encoding = \"ISO-8859-1\") as infile:\n",
    "        emails.append(infile.read())\n",
    "        labels.append(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 481)\t1\n",
      "  (0, 357)\t1\n",
      "  (0, 69)\t1\n",
      "  (0, 285)\t1\n",
      "  (0, 424)\t1\n",
      "  (0, 250)\t1\n",
      "  (0, 345)\t1\n",
      "  (0, 445)\t1\n",
      "  (0, 231)\t1\n",
      "  (0, 497)\t1\n",
      "  (0, 47)\t1\n",
      "  (0, 178)\t2\n",
      "  (0, 125)\t2\n"
     ]
    }
   ],
   "source": [
    "# preprocess and clean the raw text data, includes: \n",
    "# 1) Number and punctuation removal\n",
    "# 2) Human name removal (optional)\n",
    "# 3) Stop words removal\n",
    "# 4) Lemmatization\n",
    "def letters_only(astr):\n",
    "    return astr.isalpha()\n",
    "\n",
    "\n",
    "all_names = set(names.words())\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def clean_text(docs):\n",
    "    cleaned_docs = []\n",
    "    for doc in docs:\n",
    "        cleaned_docs.append(' '.join([lemmatizer.lemmatize(word.lower())\n",
    "                                        for word in doc.split()\n",
    "                                        if letters_only(word)\n",
    "                                        and word not in all_names]))\n",
    "    return cleaned_docs\n",
    "\n",
    "\n",
    "cv = CountVectorizer(stop_words=\"english\", max_features=500)\n",
    "\n",
    "cleaned_emails = clean_text(emails)\n",
    "term_docs = cv.fit_transform(cleaned_emails)\n",
    "print(term_docs [0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['able', 'access', 'account', 'accounting', 'act']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mapping = cv.vocabulary\n",
    "feature_names = cv.get_feature_names()\n",
    "\n",
    "def get_label_index(labels):\n",
    "    from collections import defaultdict\n",
    "    label_index = defaultdict(list)\n",
    "    for index, label in enumerate(labels):\n",
    "        label_index[label].append(index)\n",
    "    return label_index\n",
    "\n",
    "\n",
    "def get_prior(label_index):\n",
    "    \"\"\" Compute prior based on training samples\n",
    "    Args:\n",
    "        label_index (grouped sample indices by class)\n",
    "    Returns:\n",
    "        dictionary, with class label as key, corresponding prior as the value\n",
    "    \"\"\"\n",
    "    prior = {label: len(index) for label, index in label_index.items()}\n",
    "    total_count = sum(prior.values())\n",
    "    for label in prior:\n",
    "        prior[label] /= float(total_count)\n",
    "    return prior\n",
    "\n",
    "\n",
    "def get_likelihood(term_document_matrix, label_index, smoothing=0):\n",
    "    \"\"\" Compute likelihood based on training samples\n",
    "    Args:\n",
    "        term_document_matrix (sparse matrix)\n",
    "        label_index (grouped sample indices by class)\n",
    "        smoothing (integer, additive Laplace smoothing parameter)\n",
    "    Returns:\n",
    "        dictionary, with class as key, corresponding conditional probability P(feature|class) vector as value\n",
    "    \"\"\"\n",
    "    likelihood = {}\n",
    "    for label, index in label_index.items():\n",
    "        likelihood[label] = term_document_matrix[index, :].sum(axis=0) + smoothing\n",
    "        likelihood[label] = np.asarray(likelihood[label])[0]\n",
    "        total_count = likelihood[label].sum()\n",
    "        likelihood[label] = likelihood[label] / float(total_count)\n",
    "    return likelihood\n",
    "\n",
    "feature_names[:5]\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posterior(term_document_matrix, prior, likelihood):\n",
    "    \"\"\" Compute posterior of testing samples, based on prior and likelihood\n",
    "    Args:\n",
    "        term_document_matrix (sparse matrix)\n",
    "        prior (dictionary, with class label as key, corresponding prior as the value)\n",
    "        likelihood (dictionary, with class label as key, corresponding conditional probability vector as value)\n",
    "    Returns:\n",
    "        dictionary, with class label as key, corresponding posterior as value\n",
    "    \"\"\"\n",
    "    num_docs = term_document_matrix.shape[0]\n",
    "    posteriors = []\n",
    "    for i in range(num_docs):\n",
    "        # posterior is proportional to prior * likelihood\n",
    "        # = exp(log(prior * likelihood))\n",
    "        # = exp(log(prior) + log(likelihood))\n",
    "        posterior = {key: np.log(prior_label) for key, prior_label in prior.items()}\n",
    "        for label, likelihood_label in likelihood.items():\n",
    "            term_document_vector = term_document_matrix.getrow(i)\n",
    "            counts = term_document_vector.data\n",
    "            indices = term_document_vector.indices\n",
    "            for count, index in zip(counts, indices):\n",
    "                posterior[label] += np.log(likelihood_label[index]) * count\n",
    "        # exp(-1000):exp(-999) will cause zero division error,\n",
    "        # however it equates to exp(0):exp(1)\n",
    "        min_log_posterior = min(posterior.values())\n",
    "        for label in posterior:\n",
    "            try:\n",
    "                posterior[label] = np.exp(posterior[label] - min_log_posterior)\n",
    "            except:\n",
    "                # if one's log value is excessively large, assign it infinity\n",
    "                posterior[label] = float('inf')\n",
    "        # normalize so that all sums up to 1\n",
    "        sum_posterior = sum(posterior.values())\n",
    "        for label in posterior:\n",
    "            if posterior[label] == float('inf'):\n",
    "                posterior[label] = 1.0\n",
    "            else:\n",
    "                posterior[label] /= sum_posterior\n",
    "        posteriors.append(posterior.copy())\n",
    "    return posteriors\n",
    "\n",
    "\n",
    "label_index = get_label_index(labels)\n",
    "prior = get_prior(label_index)\n",
    "\n",
    "smoothing = 1\n",
    "likelihood = get_likelihood(term_docs, label_index, smoothing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{1: 0.0032745671008376, 0: 0.9967254328991624}, {1: 0.9999984725538845, 0: 1.5274461154428757e-06}]\n"
     ]
    }
   ],
   "source": [
    "emails_test = [\n",
    "    '''Subject: flat screens\n",
    "    hello ,\n",
    "    please call or contact regarding the other flat screens requested .\n",
    "    trisha tlapek - eb 3132 b\n",
    "    michael sergeev - eb 3132 a\n",
    "    also the sun blocker that was taken away from eb 3131 a .\n",
    "    trisha should two monitors also michael .\n",
    "    thanks\n",
    "    kevin moore''',\n",
    "    '''Subject: having problems in bed ? we can help !\n",
    "    cialis allows men to enjoy a fully normal sex life without having to plan the sexual act .\n",
    "    if we let things terrify us , life will not be worth living .\n",
    "    brevity is the soul of lingerie .\n",
    "    suspicion always haunts the guilty mind .''',\n",
    "]\n",
    "\n",
    "cleaned_test = clean_text(emails_test)\n",
    "term_docs_test = cv.transform(cleaned_test)\n",
    "posterior = get_posterior(term_docs_test, prior, likelihood)\n",
    "print(posterior)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on 1707 testing samples is: 92.0%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(cleaned_emails, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "len(X_train), len(Y_train)\n",
    "len(X_test), len(Y_test)\n",
    "\n",
    "term_docs_train = cv.fit_transform(X_train)\n",
    "label_index = get_label_index(Y_train)\n",
    "prior = get_prior(label_index)\n",
    "likelihood = get_likelihood(term_docs_train, label_index, smoothing)\n",
    "\n",
    "term_docs_test = cv.transform(X_test)\n",
    "posterior = get_posterior(term_docs_test, prior, likelihood)\n",
    "\n",
    "correct = 0.0\n",
    "for pred, actual in zip(posterior, Y_test):\n",
    "    if actual == 1:\n",
    "        if pred[1] >= 0.5:\n",
    "            correct += 1\n",
    "    elif pred[0] > 0.5:\n",
    "        correct += 1\n",
    "\n",
    "print('The accuracy on {0} testing samples is: {1:.1f}%'.format(len(Y_test), correct/len(Y_test)*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy using MultinomialNB is: 92.0%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=1.0, fit_prior=True)\n",
    "clf.fit(term_docs_train, Y_train)\n",
    "prediction_prob = clf.predict_proba(term_docs_test)\n",
    "prediction_prob[0:10]\n",
    "prediction = clf.predict(term_docs_test)\n",
    "prediction[:10]\n",
    "accuracy = clf.score(term_docs_test, Y_test)\n",
    "print('The accuracy using MultinomialNB is: {0:.1f}%'.format(accuracy*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94      1191\n",
      "           1       0.84      0.92      0.87       516\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      1707\n",
      "   macro avg       0.90      0.92      0.91      1707\n",
      "weighted avg       0.92      0.92      0.92      1707\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classifier performance evaluation - Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Y_test, prediction, labels=[0, 1])\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "precision_score(Y_test, prediction, pos_label=1)\n",
    "recall_score(Y_test, prediction, pos_label=1)\n",
    "f1_score(Y_test, prediction, pos_label=1)\n",
    "\n",
    "f1_score(Y_test, prediction, pos_label=0)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Y_test, prediction)\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_prob = prediction_prob[:, 1]\n",
    "thresholds = np.arange(0.0, 1.2, 0.1)\n",
    "true_pos, false_pos = [0]*len(thresholds), [0]*len(thresholds)\n",
    "for pred, y in zip(pos_prob, Y_test):\n",
    "    for i, threshold in enumerate(thresholds):\n",
    "        if pred >= threshold:\n",
    "            if y == 1:\n",
    "                true_pos[i] += 1\n",
    "            else:\n",
    "                false_pos[i] += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "true_pos_rate = [tp / 516.0 for tp in true_pos]\n",
    "false_pos_rate = [fp / 1191.0 for fp in false_pos]\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(false_pos_rate, true_pos_rate, color='darkorange',\n",
    "         lw=lw)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "#plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9582877719849778"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(Y_test, pos_prob)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2000: {0.5: {True: 9.744341507720254, False: 9.743687186549776}, 1.0: {True: 9.726073579354736, False: 9.725047017533468}, 1.5: {True: 9.7146733206966, False: 9.715017869130829}, 2.0: {True: 9.706112180626308, False: 9.706747324566601}}, 4000: {0.5: {True: 9.81694519310508, False: 9.814603892706236}, 1.0: {True: 9.796673651423607, False: 9.797172678987483}, 1.5: {True: 9.785206778422777, False: 9.786758875330728}, 2.0: {True: 9.778234090550091, False: 9.77867877028788}}, 8000: {0.5: {True: 9.85627517474233, False: 9.854758174386921}, 1.0: {True: 9.845380632231569, False: 9.845271097421316}, 1.5: {True: 9.840752033724282, False: 9.841142390317103}, 2.0: {True: 9.837345101291318, False: 9.837871672985033}}}\n",
      "max features  smoothing  fit prior  auc\n",
      "       2000      0.5      True    0.9744\n",
      "       2000      0.5      False    0.9744\n",
      "       2000      1.0      True    0.9726\n",
      "       2000      1.0      False    0.9725\n",
      "       2000      1.5      True    0.9715\n",
      "       2000      1.5      False    0.9715\n",
      "       2000      2.0      True    0.9706\n",
      "       2000      2.0      False    0.9707\n",
      "       4000      0.5      True    0.9817\n",
      "       4000      0.5      False    0.9815\n",
      "       4000      1.0      True    0.9797\n",
      "       4000      1.0      False    0.9797\n",
      "       4000      1.5      True    0.9785\n",
      "       4000      1.5      False    0.9787\n",
      "       4000      2.0      True    0.9778\n",
      "       4000      2.0      False    0.9779\n",
      "       8000      0.5      True    0.9856\n",
      "       8000      0.5      False    0.9855\n",
      "       8000      1.0      True    0.9845\n",
      "       8000      1.0      False    0.9845\n",
      "       8000      1.5      True    0.9841\n",
      "       8000      1.5      False    0.9841\n",
      "       8000      2.0      True    0.9837\n",
      "       8000      2.0      False    0.9838\n"
     ]
    }
   ],
   "source": [
    "# Model tuning and cross-validation - k-fold cross-validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "k = 10\n",
    "k_fold = StratifiedKFold(n_splits=k)\n",
    "# convert to numpy array for more efficient slicing\n",
    "cleaned_emails_np = np.array(cleaned_emails)\n",
    "labels_np = np.array(labels)\n",
    "\n",
    "max_features_option = [2000, 4000, 8000]\n",
    "smoothing_factor_option = [0.5, 1.0, 1.5, 2.0]\n",
    "fit_prior_option = [True, False]\n",
    "auc_record = {}\n",
    "\n",
    "for train_indices, test_indices in k_fold.split(cleaned_emails, labels):\n",
    "    X_train, X_test = cleaned_emails_np[train_indices], cleaned_emails_np[test_indices]\n",
    "    Y_train, Y_test = labels_np[train_indices], labels_np[test_indices]\n",
    "    for max_features in max_features_option:\n",
    "        if max_features not in auc_record:\n",
    "            auc_record[max_features] = {}\n",
    "        cv = CountVectorizer(stop_words=\"english\", max_features=max_features)\n",
    "        term_docs_train = cv.fit_transform(X_train)\n",
    "        term_docs_test = cv.transform(X_test)\n",
    "        for smoothing_factor in smoothing_factor_option:\n",
    "            if smoothing_factor not in auc_record[max_features]:\n",
    "                auc_record[max_features][smoothing_factor] = {}\n",
    "            for fit_prior in fit_prior_option:\n",
    "                clf = MultinomialNB(alpha=smoothing_factor, fit_prior=fit_prior)\n",
    "                clf.fit(term_docs_train, Y_train)\n",
    "                prediction_prob = clf.predict_proba(term_docs_test)\n",
    "                pos_prob = prediction_prob[:, 1]\n",
    "                auc = roc_auc_score(Y_test, pos_prob)\n",
    "                auc_record[max_features][smoothing_factor][fit_prior] \\\n",
    "                    = auc + auc_record[max_features][smoothing_factor].get(fit_prior, 0.0)\n",
    "\n",
    "print(auc_record)\n",
    "\n",
    "print('max features  smoothing  fit prior  auc')\n",
    "for max_features, max_feature_record in auc_record.items():\n",
    "    for smoothing, smoothing_record in max_feature_record.items():\n",
    "        for fit_prior, auc in smoothing_record.items():\n",
    "            print('       {0}      {1}      {2}    {3:.4f}'.format(max_features, smoothing, fit_prior, auc/k))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'float'>, {1.0: 9.919583333333334, 2.0: 9.929585603996365, 3.0: 9.935521512064131, 4.0: 9.93993197883347, 5.0: 9.942509526912293})\n",
      "max features  smoothing  fit prior  auc\n",
      "       8000      1.0      true    0.9920\n",
      "       8000      2.0      true    0.9930\n",
      "       8000      3.0      true    0.9936\n",
      "       8000      4.0      true    0.9940\n",
      "       8000      5.0      true    0.9943\n"
     ]
    }
   ],
   "source": [
    "# Apply k-fold for validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "k = 10\n",
    "k_fold = StratifiedKFold(n_splits=k)\n",
    "# convert to numpy array for more efficient slicing\n",
    "cleaned_emails_np = np.array(cleaned_emails)\n",
    "labels_np = np.array(labels)\n",
    "\n",
    "smoothing_factor_option = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "from collections import defaultdict\n",
    "auc_record = defaultdict(float)\n",
    "\n",
    "for train_indices, test_indices in k_fold.split(cleaned_emails, labels):\n",
    "    X_train, X_test = cleaned_emails_np[train_indices], cleaned_emails_np[test_indices]\n",
    "    Y_train, Y_test = labels_np[train_indices], labels_np[test_indices]\n",
    "    tfidf_vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english', max_features=8000)\n",
    "    term_docs_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "    term_docs_test = tfidf_vectorizer.transform(X_test)\n",
    "    for smoothing_factor in smoothing_factor_option:\n",
    "        clf = MultinomialNB(alpha=smoothing_factor, fit_prior=True)\n",
    "        clf.fit(term_docs_train, Y_train)\n",
    "        prediction_prob = clf.predict_proba(term_docs_test)\n",
    "        pos_prob = prediction_prob[:, 1]\n",
    "        auc = roc_auc_score(Y_test, pos_prob)\n",
    "        auc_record[smoothing_factor] += auc\n",
    "\n",
    "print(auc_record)\n",
    "\n",
    "print('max features  smoothing  fit prior  auc')\n",
    "for smoothing, smoothing_record in auc_record.items():\n",
    "        print('       8000      {0}      true    {1:.4f}'.format(smoothing, smoothing_record/k))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
